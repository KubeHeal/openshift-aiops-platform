{{- if and .Values.modelServing.enabled .Values.models }}
{{- /*
  Post-Training Restart Job for InferenceService Predictors

  Purpose: Fixes race condition where predictor pods start (wave 2) before
           models are trained and written to PVC (wave 3+).

  Issue: KServe sklearn server loads models only at startup and does not retry.
         Predictors report Ready: True even when models are missing, causing
         ModelMissingError on inference requests.

  Solution: This job runs at sync-wave 11 (after all notebook validation jobs
            complete at waves 0-10) and:
            1. Waits for model.pkl files to exist on PVC
            2. Restarts predictor deployments to trigger model reload
            3. Validates pods reach Running state after restart

  Sync Wave Timeline:
    wave 2:  InferenceServices created (predictors start, no models yet)
    wave 3:  isolation-forest + predictive-analytics-kserve notebooks train models
    wave 4-10: remaining notebooks
    wave 11: This restart Job -- restarts predictors so they reload models

  Reference: Issue #34, ADR-054
*/ -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: model-restart-after-training
  namespace: {{ .Values.main.namespace }}
  labels:
    app.kubernetes.io/component: model-serving
    app.kubernetes.io/name: model-restart-job
    app.kubernetes.io/part-of: self-healing-platform
  annotations:
    argocd.argoproj.io/sync-wave: "11"
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
    description: "Waits for trained models to exist, then restarts InferenceService predictor pods"
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: model-serving
        app.kubernetes.io/name: model-restart-job
    spec:
      serviceAccountName: self-healing-operator
      restartPolicy: OnFailure
      initContainers:
      # Wait for PVCs to be mounted
      - name: wait-for-pvcs
        image: registry.access.redhat.com/ubi9/ubi-minimal:latest
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Waiting for PVCs to be mounted..."
          MAX_WAIT=300  # 5 minutes
          ELAPSED=0
          INTERVAL=10

          # Wait for final PVC (required)
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            if [ -d /mnt/models ]; then
              echo "‚úì Final PVC (model-storage-pvc) is mounted successfully!"
              break
            fi
            echo "Final PVC not yet available, waiting ${INTERVAL}s... (${ELAPSED}s/${MAX_WAIT}s)"
            sleep $INTERVAL
            ELAPSED=$((ELAPSED + INTERVAL))
          done

          if [ ! -d /mnt/models ]; then
            echo "ERROR: Final PVC did not become available within ${MAX_WAIT}s"
            exit 1
          fi

          # Check for GPU PVC (optional - may not exist for non-GPU models)
          if [ -d /mnt/models-gpu ]; then
            echo "‚úì GPU PVC (model-storage-gpu-pvc) is also mounted"
          else
            echo "‚ÑπÔ∏è  GPU PVC not mounted (not needed for this model or not available)"
          fi

          echo "‚úì All required PVCs are ready"
        volumeMounts:
        - name: model-storage
          mountPath: /mnt/models
        - name: model-storage-gpu
          mountPath: /mnt/models-gpu
      containers:
      - name: restart-predictors
        image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest
        command:
        - /bin/bash
        - -c
        - |
          set -e
          NAMESPACE="{{ .Values.main.namespace }}"
          MAX_WAIT_PER_MODEL=600  # 10 minutes per model
          POLL_INTERVAL=10
          RESTART_TIMEOUT=300  # 5 minutes for pod restart

          echo "=========================================="
          echo "Restart Predictors After Models Ready"
          echo "=========================================="
          echo "Namespace: $NAMESPACE"
          echo "Max wait per model: ${MAX_WAIT_PER_MODEL}s"
          echo "Poll interval: ${POLL_INTERVAL}s"
          echo "=========================================="

          # Function to copy model from GPU PVC to final PVC (if needed)
          copy_model_from_gpu() {
            local model_name=$1
            local gpu_path="/mnt/models-gpu/${model_name}/model.pkl"
            local final_path="/mnt/models/${model_name}/model.pkl"
            local final_dir="/mnt/models/${model_name}"

            # Check if GPU PVC is available and has model
            if [ ! -d /mnt/models-gpu ]; then
              echo "‚ÑπÔ∏è  GPU PVC not mounted (not needed for this model)"
              return 0
            fi

            if [ ! -f "$gpu_path" ]; then
              echo "‚ÑπÔ∏è  Model not found in GPU PVC (not GPU-trained)"
              return 0
            fi

            # Check if already in final PVC
            if [ -f "$final_path" ]; then
              echo "‚ÑπÔ∏è  Model already exists in final PVC, skipping copy"
              return 0
            fi

            echo "üîÑ Copying model from GPU PVC to final PVC..."
            echo "   Source: $gpu_path"
            echo "   Target: $final_path"

            # Create target directory
            mkdir -p "$final_dir"

            # Copy model file
            cp "$gpu_path" "$final_path"

            # Verify copy
            if [ -f "$final_path" ]; then
              local source_size=$(stat -f%z "$gpu_path" 2>/dev/null || stat -c%s "$gpu_path" 2>/dev/null || echo "0")
              local target_size=$(stat -f%z "$final_path" 2>/dev/null || stat -c%s "$final_path" 2>/dev/null || echo "0")
              if [ "$source_size" = "$target_size" ] && [ "$source_size" != "0" ]; then
                echo "‚úì Model copied successfully! (size: $source_size bytes)"
                return 0
              else
                echo "‚úó ERROR: Copy size mismatch (source: $source_size, target: $target_size)"
                return 1
              fi
            else
              echo "‚úó ERROR: Copy failed - target file not found"
              return 1
            fi
          }

          # Function to wait for model file
          wait_for_model() {
            local model_name=$1
            local model_path="/mnt/models/${model_name}/model.pkl"
            local elapsed=0

            echo ""
            echo "Waiting for model: $model_name"
            echo "Expected path: $model_path"

            # First, try copying from GPU PVC if available
            copy_model_from_gpu "$model_name"

            # Then wait for model in final PVC
            while [ $elapsed -lt $MAX_WAIT_PER_MODEL ]; do
              if [ -f "$model_path" ]; then
                local file_size=$(stat -f%z "$model_path" 2>/dev/null || stat -c%s "$model_path" 2>/dev/null || echo "unknown")
                echo "‚úì Model file found: $model_path (size: $file_size bytes)"
                return 0
              fi

              echo "‚è≥ Model not found, waiting ${POLL_INTERVAL}s... (${elapsed}s/${MAX_WAIT_PER_MODEL}s)"
              sleep $POLL_INTERVAL
              elapsed=$((elapsed + POLL_INTERVAL))
            done

            echo "‚úó ERROR: Model file not found after ${MAX_WAIT_PER_MODEL}s: $model_path"
            return 1
          }

          # Function to restart predictor deployment
          restart_predictor() {
            local service_name=$1
            local deployment_name="${service_name}-predictor"

            echo ""
            echo "Restarting predictor for InferenceService: $service_name"
            echo "Deployment name: $deployment_name"

            # Get current pod count
            local pod_count=$(oc get pods -n "$NAMESPACE" \
              -l serving.kserve.io/inferenceservice="$service_name" \
              --no-headers 2>/dev/null | wc -l | tr -d ' ')

            if [ "$pod_count" -eq 0 ]; then
              echo "‚ö†Ô∏è  WARNING: No predictor pods found for $service_name, skipping restart"
              return 0
            fi

            echo "Current pod count: $pod_count"

            # Restart deployment
            if oc rollout restart deployment/"$deployment_name" -n "$NAMESPACE" 2>/dev/null; then
              echo "‚úì Deployment restart triggered"
            else
              echo "‚ö†Ô∏è  WARNING: Could not restart deployment $deployment_name (may not exist yet)"
              # Try deleting pods directly as fallback
              echo "Attempting direct pod deletion..."
              oc delete pods -n "$NAMESPACE" \
                -l serving.kserve.io/inferenceservice="$service_name" \
                --wait=false 2>/dev/null || true
            fi

            # Wait for pods to be running
            echo "Waiting for pods to restart..."
            local elapsed=0
            while [ $elapsed -lt $RESTART_TIMEOUT ]; do
              local ready_count=$(oc get pods -n "$NAMESPACE" \
                -l serving.kserve.io/inferenceservice="$service_name" \
                --field-selector=status.phase=Running \
                --no-headers 2>/dev/null | wc -l | tr -d ' ')

              if [ "$ready_count" -ge "$pod_count" ]; then
                echo "‚úì All pods are running ($ready_count/$pod_count)"
                oc get pods -n "$NAMESPACE" -l serving.kserve.io/inferenceservice="$service_name"
                return 0
              fi

              echo "‚è≥ Waiting for pods... ($ready_count/$pod_count running, elapsed: ${elapsed}s)"
              sleep 5
              elapsed=$((elapsed + 5))
            done

            echo "‚ö†Ô∏è  WARNING: Timeout waiting for pods to restart (${elapsed}s/${RESTART_TIMEOUT}s)"
            oc get pods -n "$NAMESPACE" -l serving.kserve.io/inferenceservice="$service_name"
            return 1
          }

          # Process each model
          {{- range .Values.models }}
          MODEL_NAME="{{ .name }}"
          echo ""
          echo "=========================================="
          echo "Processing model: $MODEL_NAME"
          echo "=========================================="

          # Wait for model file
          if ! wait_for_model "$MODEL_NAME"; then
            echo "‚úó Failed to find model for $MODEL_NAME, skipping restart"
            continue
          fi

          # Restart predictor
          if restart_predictor "$MODEL_NAME"; then
            echo "‚úì Successfully restarted predictor for $MODEL_NAME"
          else
            echo "‚ö†Ô∏è  Warning: Restart may not have completed for $MODEL_NAME"
          fi
          {{- end }}

          echo ""
          echo "=========================================="
          echo "‚úì‚úì‚úì All predictors restarted successfully ‚úì‚úì‚úì"
          echo "=========================================="
        volumeMounts:
        - name: model-storage
          mountPath: /mnt/models
        - name: model-storage-gpu
          mountPath: /mnt/models-gpu
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-storage-pvc
      - name: model-storage-gpu
        persistentVolumeClaim:
          claimName: model-storage-gpu-pvc
  backoffLimit: 3
  activeDeadlineSeconds: 1800  # 30 minutes total timeout
{{- end }}
