{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest Anomaly Detection for Self-Healing Platform\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates implementing Isolation Forest for anomaly detection in OpenShift metrics. Isolation Forest is particularly effective for detecting anomalies in high-dimensional data without requiring labeled training data.\n",
    "\n",
    "## Prerequisites\n",
    "- Completed: `synthetic-anomaly-generation.ipynb` (Phase 1)\n",
    "- PyTorch workbench environment with scikit-learn\n",
    "- Synthetic dataset: `/opt/app-root/src/data/processed/synthetic_anomalies.parquet`\n",
    "\n",
    "## Why We Use Synthetic Data\n",
    "\n",
    "### The Problem: Real Anomalies Are Rare\n",
    "In production OpenShift clusters:\n",
    "- Anomalies occur <1% of the time\n",
    "- Collecting 1000 labeled anomalies takes months/years\n",
    "- Different anomaly types are hard to capture\n",
    "- Can't deliberately cause failures to collect data\n",
    "\n",
    "### The Solution: Synthetic Anomalies\n",
    "We generate synthetic anomalies because:\n",
    "- ‚úÖ Create 1000+ labeled anomalies in minutes\n",
    "- ‚úÖ Control anomaly types and severity\n",
    "- ‚úÖ Ensure balanced training data (50% normal, 50% anomaly)\n",
    "- ‚úÖ Reproducible and testable\n",
    "- ‚úÖ Models trained on synthetic data generalize to real anomalies\n",
    "\n",
    "### Machine Learning Best Practice\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "## Enhanced Metrics (v2.0)\n",
    "This version includes **30+ metrics** across 6 categories:\n",
    "- **CPU**: Utilization, saturation, iowait, steal, throttling\n",
    "- **Memory**: Utilization, pressure, OOM kills, swap\n",
    "- **Disk I/O**: Latency, IOPS, throughput, utilization\n",
    "- **Network**: Errors, drops, retransmits, conntrack\n",
    "- **Stability**: Restarts, crashes, pending pods\n",
    "- **Kubernetes State**: Deployments, nodes, quotas\n",
    "\n",
    "## Expected Outcomes\n",
    "- Train Isolation Forest model on synthetic anomalies\n",
    "- Evaluate model performance (Precision, Recall, F1)\n",
    "- Save trained model for integration with coordination engine\n",
    "- Generate anomaly detection pipeline for real-time use\n",
    "\n",
    "## References\n",
    "- ADR-002: Hybrid Deterministic-AI Self-Healing Approach\n",
    "- ADR-012: Notebook Architecture for End-to-End Workflows\n",
    "- [Isolation Forest Paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf) - Liu, Ting & Zhou (2008)\n",
    "- [Learning from Imbalanced Data](https://ieeexplore.ieee.org/document/5128907) - He & Garcia (2009)\n",
    "- [Anomaly Detection with Robust Deep Autoencoders](https://arxiv.org/abs/1511.08747) - Goldstein & Uchida (2016)"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport os\nfrom pathlib import Path\n\n# Setup path for utils module - works from any directory\ndef find_utils_path():\n    \"\"\"Find utils path regardless of current working directory\"\"\"\n    possible_paths = [\n        Path(__file__).parent.parent / 'utils' if '__file__' in dir() else None,\n        Path.cwd() / 'notebooks' / 'utils',\n        Path.cwd().parent / 'utils',\n        Path('/workspace/repo/notebooks/utils'),\n        Path('/opt/app-root/src/notebooks/utils'),\n        Path('/opt/app-root/src/openshift-aiops-platform/notebooks/utils'),\n    ]\n    for p in possible_paths:\n        if p and p.exists() and (p / 'common_functions.py').exists():\n            return str(p)\n    current = Path.cwd()\n    for _ in range(5):\n        utils_path = current / 'notebooks' / 'utils'\n        if utils_path.exists():\n            return str(utils_path)\n        current = current.parent\n    return None\n\nutils_path = find_utils_path()\nif utils_path:\n    sys.path.insert(0, utils_path)\n    print(f\"‚úÖ Utils path found: {utils_path}\")\nelse:\n    print(\"‚ö†Ô∏è Utils path not found - will use fallback implementations\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine learning libraries\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline  # ‚ú® Added for KServe compatibility\n\n# Try to import common functions, with fallback\ntry:\n    from common_functions import (\n        setup_environment, print_environment_info,\n        generate_synthetic_timeseries, validate_data_quality,\n        plot_metric_overview, save_processed_data, load_processed_data\n    )\n    print(\"‚úÖ Common functions imported\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Common functions not available: {e}\")\n    print(\"   Using minimal fallback implementations\")\n    \n    def setup_environment():\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        os.makedirs('/opt/app-root/src/models/anomaly-detection', exist_ok=True)\n        return {'data_dir': '/opt/app-root/src/data', 'models_dir': '/opt/app-root/src/models'}\n    \n    def print_environment_info(env_info):\n        print(f\"üìÅ Data dir: {env_info.get('data_dir', 'N/A')}\")\n    \n    def generate_synthetic_timeseries(metric_name, duration_hours=24, interval_minutes=1, \n                                      add_anomalies=True, anomaly_probability=0.02):\n        num_points = int(duration_hours * 60 / interval_minutes)\n        timestamps = pd.date_range(end=datetime.now(), periods=num_points, freq=f'{interval_minutes}min')\n        values = np.random.normal(50, 10, num_points)\n        if add_anomalies:\n            anomaly_idx = np.random.choice(num_points, int(num_points * anomaly_probability), replace=False)\n            values[anomaly_idx] *= np.random.choice([0.3, 3.0], len(anomaly_idx))\n        df = pd.DataFrame({'timestamp': timestamps, 'value': values, 'metric': metric_name, 'is_anomaly': False})\n        if add_anomalies:\n            df.loc[anomaly_idx, 'is_anomaly'] = True\n        return df\n    \n    def save_processed_data(data, filename):\n        os.makedirs('/opt/app-root/src/data/processed', exist_ok=True)\n        filepath = f'/opt/app-root/src/data/processed/{filename}'\n        if hasattr(data, 'to_parquet'):\n            data.to_parquet(filepath)\n        print(f\"üíæ Saved: {filepath}\")\n\nprint(\"‚úÖ Libraries imported successfully\")\nprint(f\"üî¨ Scikit-learn available with Pipeline support\")\nprint(f\"üìä Pandas version: {pd.__version__}\")",
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Metrics Configuration\n",
    "\n",
    "Import the enhanced metrics configuration module which provides:\n",
    "- **30+ metrics** across 6 categories (vs. original 5)\n",
    "- **Category-specific** Isolation Forest configurations\n",
    "- **Pre-defined PromQL queries** for each metric\n",
    "- **Thresholds** for warning and critical alerts"
   ],
   "id": "cell-2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ENHANCED METRICS CONFIGURATION (v2.0)\n# ============================================================================\n# Import enhanced metrics module\n# If not available, fall back to inline definitions\n\ntry:\n    from enhanced_metrics_config import (\n        ISOLATION_FOREST_CONFIGS,\n        AnomalyCategory,\n        TARGET_METRICS_ENHANCED,\n        STABILITY_METRICS,\n        PERFORMANCE_METRICS,\n        RESOURCE_EXHAUSTION_METRICS,\n        get_prometheus_queries,\n        get_thresholds,\n    )\n    ENHANCED_CONFIG_AVAILABLE = True\n    print(\"‚úÖ Enhanced metrics configuration loaded from module\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è Enhanced metrics module not found - using inline configuration\")\n    ENHANCED_CONFIG_AVAILABLE = False\n    \n    # Inline fallback: Define enhanced configuration directly\n    from enum import Enum\n    \n    class AnomalyCategory(Enum):\n        RESOURCE = \"resource\"\n        STABILITY = \"stability\"\n        PERFORMANCE = \"performance\"\n        NETWORK = \"network\"\n        CONTROL_PLANE = \"control_plane\"\n    \n    ISOLATION_FOREST_CONFIGS = {\n        AnomalyCategory.RESOURCE: {\n            'contamination': 0.05,\n            'n_estimators': 200,\n            'max_samples': 'auto',\n            'max_features': 1.0,\n            'random_state': 42,\n            'bootstrap': False,\n            'n_jobs': -1\n        },\n        AnomalyCategory.STABILITY: {\n            'contamination': 0.03,\n            'n_estimators': 150,\n            'max_samples': 256,\n            'max_features': 0.8,\n            'random_state': 42,\n            'bootstrap': True,\n            'n_jobs': -1\n        },\n        AnomalyCategory.PERFORMANCE: {\n            'contamination': 0.08,\n            'n_estimators': 200,\n            'max_samples': 'auto',\n            'max_features': 1.0,\n            'random_state': 42,\n            'bootstrap': False,\n            'n_jobs': -1\n        },\n        AnomalyCategory.NETWORK: {\n            'contamination': 0.06,\n            'n_estimators': 175,\n            'max_samples': 'auto',\n            'max_features': 0.9,\n            'random_state': 42,\n            'bootstrap': False,\n            'n_jobs': -1\n        },\n        AnomalyCategory.CONTROL_PLANE: {\n            'contamination': 0.02,\n            'n_estimators': 250,\n            'max_samples': 512,\n            'max_features': 1.0,\n            'random_state': 42,\n            'bootstrap': False,\n            'n_jobs': -1\n        }\n    }\n    \n    TARGET_METRICS_ENHANCED = [\n        # Original metrics\n        'node_cpu_utilization',\n        'node_memory_utilization',\n        'pod_cpu_usage',\n        'pod_memory_usage',\n        'container_restart_count',\n        # CPU enhancements\n        'node_cpu_saturation',\n        'node_cpu_iowait',\n        'pod_cpu_throttled_percent',\n        'node_load_per_cpu',\n        # Memory enhancements  \n        'node_memory_pressure',\n        'node_memory_oom_kills',\n        'pod_memory_utilization',\n        # Disk I/O\n        'node_disk_io_utilization',\n        'node_disk_read_latency_ms',\n        'node_disk_write_latency_ms',\n        # Network\n        'node_network_errors',\n        'node_network_drops',\n        'node_tcp_retransmit_rate',\n        'pod_network_errors',\n        # Kubernetes state\n        'pods_pending',\n        'pods_not_ready',\n        'deployment_replicas_unavailable',\n        # Stability\n        'container_restart_rate_1h',\n        'pod_crash_loop_backoff',\n        'pod_oom_killed',\n    ]\n    \n    STABILITY_METRICS = [\n        'container_restart_count', 'container_restart_rate_1h',\n        'pod_crash_loop_backoff', 'pod_oom_killed',\n        'pods_pending', 'pods_not_ready', 'pods_failed',\n        'deployment_replicas_unavailable', 'node_memory_oom_kills',\n    ]\n\nprint(f\"üìä Enhanced metrics available: {len(TARGET_METRICS_ENHANCED)}\")",
   "id": "cell-2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up environment\nenv_info = setup_environment()\nprint_environment_info(env_info)\n\n# ============================================================================\n# DETECTION FOCUS SELECTION\n# ============================================================================\n# Choose your detection focus based on what anomalies you want to catch:\n#\n#   AnomalyCategory.RESOURCE      - CPU/memory exhaustion (most common)\n#   AnomalyCategory.STABILITY     - Crashes, restarts, OOM kills\n#   AnomalyCategory.PERFORMANCE   - Latency spikes, throughput issues\n#   AnomalyCategory.NETWORK       - Connectivity, packet loss, errors\n#   AnomalyCategory.CONTROL_PLANE - API server, etcd, scheduler issues\n#\n# Each category has a tuned Isolation Forest configuration optimized for\n# that type of anomaly detection.\n# ============================================================================\n\nDETECTION_FOCUS = AnomalyCategory.RESOURCE  # üëà Change this to switch focus\n\n# Get category-specific Isolation Forest configuration\nISOLATION_FOREST_CONFIG = ISOLATION_FOREST_CONFIGS[DETECTION_FOCUS]\n\nprint(f\"\\nüéØ Detection Focus: {DETECTION_FOCUS.value.upper()}\")\nprint(f\"   Contamination: {ISOLATION_FOREST_CONFIG['contamination']} ({ISOLATION_FOREST_CONFIG['contamination']*100:.0f}% expected anomalies)\")\nprint(f\"   Estimators: {ISOLATION_FOREST_CONFIG['n_estimators']} trees\")\nprint(f\"   Max Samples: {ISOLATION_FOREST_CONFIG['max_samples']}\")\nprint(f\"   Bootstrap: {ISOLATION_FOREST_CONFIG.get('bootstrap', False)}\")\n\n# ============================================================================\n# ENHANCED TARGET METRICS\n# ============================================================================\n# Use all 30+ enhanced metrics for comprehensive anomaly detection\n# Or select a subset based on your focus area\n# ============================================================================\n\n# Option 1: Use ALL enhanced metrics (recommended for general detection)\nTARGET_METRICS = TARGET_METRICS_ENHANCED\n\n# Option 2: Use stability-focused subset\n# TARGET_METRICS = STABILITY_METRICS\n\n# Option 3: Use original 5 metrics (for comparison/baseline)\n# TARGET_METRICS = [\n#     'node_cpu_utilization',\n#     'node_memory_utilization', \n#     'pod_cpu_usage',\n#     'pod_memory_usage',\n#     'container_restart_count'\n# ]\n\nprint(f\"\\nüìä Target Metrics: {len(TARGET_METRICS)} metrics\")\nprint(f\"üå≤ Isolation Forest: {ISOLATION_FOREST_CONFIG['n_estimators']} trees\")\n\n# Display metric categories\nprint(f\"\\nüìã Metrics by category:\")\ncategories = {\n    'CPU': [m for m in TARGET_METRICS if 'cpu' in m.lower()],\n    'Memory': [m for m in TARGET_METRICS if 'memory' in m.lower() or 'oom' in m.lower()],\n    'Disk': [m for m in TARGET_METRICS if 'disk' in m.lower()],\n    'Network': [m for m in TARGET_METRICS if 'network' in m.lower() or 'tcp' in m.lower()],\n    'Stability': [m for m in TARGET_METRICS if any(x in m.lower() for x in ['restart', 'crash', 'pending', 'ready', 'failed'])],\n}\nfor cat, metrics in categories.items():\n    if metrics:\n        print(f\"   {cat}: {len(metrics)} metrics\")",
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Load Synthetic Anomalies for Training\n",
    "\n",
    "We load synthetic anomalies from Phase 1 (`synthetic-anomaly-generation.ipynb`) for training.\n",
    "\n",
    "**Why Synthetic Data?**\n",
    "- Real anomalies are rare (<1% in production clusters)\n",
    "- Synthetic data provides labeled training examples\n",
    "- Models learn general patterns, not memorize specific examples\n",
    "- Balanced dataset (50% normal, 50% anomaly) improves performance\n",
    "- Reproducible and testable\n",
    "\n",
    "**Machine Learning Best Practice:**\n",
    "Supervised learning requires labeled data. Synthetic data provides:\n",
    "1. **Ground Truth**: Known labels for evaluation\n",
    "2. **Balanced Classes**: Equal normal and anomaly samples\n",
    "3. **Reproducibility**: Same data for consistent results\n",
    "4. **Generalization**: Models learn patterns, not memorize examples\n",
    "\n",
    "**References:**\n",
    "- He & Garcia (2009): \"Learning from Imbalanced Data\" - https://ieeexplore.ieee.org/document/5128907\n",
    "- Nikolenko (2021): \"Synthetic Data for Deep Learning\" - https://arxiv.org/abs/1909.11373\n",
    "- Goldstein & Uchida (2016): \"Anomaly Detection with Robust Deep Autoencoders\" - https://arxiv.org/abs/1511.08747"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_anomaly_detection_data(duration_hours=48):\n    \"\"\"\n    Generate and prepare data for anomaly detection training.\n    \n    Now supports 30+ enhanced metrics with realistic patterns.\n    \"\"\"\n    print(\"üîÑ Preparing anomaly detection dataset...\")\n    print(f\"   Using {len(TARGET_METRICS)} enhanced metrics\")\n    \n    # Generate synthetic data for each target metric\n    all_data = {}\n    \n    for i, metric in enumerate(TARGET_METRICS):\n        print(f\"  üìä [{i+1}/{len(TARGET_METRICS)}] Generating {metric}...\")\n        \n        # Adjust anomaly probability based on metric type\n        if any(x in metric for x in ['restart', 'crash', 'oom', 'failed']):\n            anomaly_prob = 0.02  # Rare events\n        elif any(x in metric for x in ['error', 'drop', 'pending']):\n            anomaly_prob = 0.04  # Somewhat rare\n        else:\n            anomaly_prob = 0.03  # Default\n        \n        df = generate_synthetic_timeseries(\n            metric_name=metric,\n            duration_hours=duration_hours,\n            interval_minutes=1,\n            add_anomalies=True,\n            anomaly_probability=anomaly_prob\n        )\n        all_data[metric] = df\n        \n        if (i + 1) % 10 == 0:\n            print(f\"       ‚úÖ Generated {i+1}/{len(TARGET_METRICS)} metrics\")\n    \n    print(f\"\\n‚úÖ Generated data for {len(all_data)} metrics\")\n    return all_data\n\n# Generate training data\ntraining_data = prepare_anomaly_detection_data(duration_hours=48)\n\n# Display summary\ntotal_points = sum(len(df) for df in training_data.values())\ntotal_anomalies = sum(df['is_anomaly'].sum() for df in training_data.values())\nprint(f\"\\nüìà Dataset Summary:\")\nprint(f\"  Total data points: {total_points:,}\")\nprint(f\"  Total anomalies: {total_anomalies:,} ({total_anomalies/total_points:.2%})\")\nprint(f\"  Metrics: {len(training_data)}\")",
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_feature_matrix(data_dict):\n    \"\"\"\n    Create feature matrix for anomaly detection.\n    \n    Enhanced version with additional engineered features:\n    - Rolling statistics (mean, std, min, max)\n    - Lag features\n    - Rate of change\n    - Cross-metric correlations (for enhanced metrics)\n    \"\"\"\n    print(\"üîß Creating feature matrix...\")\n    print(f\"   Input: {len(data_dict)} metrics\")\n    \n    # Align all time series to common timestamps\n    min_start = max(df['timestamp'].min() for df in data_dict.values())\n    max_end = min(df['timestamp'].max() for df in data_dict.values())\n    \n    print(f\"  üìÖ Time range: {min_start} to {max_end}\")\n    \n    # Create common time index\n    time_index = pd.date_range(start=min_start, end=max_end, freq='1min')\n    \n    # Build feature matrix\n    features = pd.DataFrame(index=time_index)\n    labels = pd.Series(index=time_index, dtype=bool, name='is_anomaly')\n    \n    for metric_name, df in data_dict.items():\n        # Resample to common time index\n        df_resampled = df.set_index('timestamp').reindex(time_index, method='nearest')\n        \n        # Add basic features\n        features[f'{metric_name}_value'] = df_resampled['value']\n        \n        # Add rolling statistics (5-minute windows)\n        features[f'{metric_name}_mean_5m'] = df_resampled['value'].rolling('5min').mean()\n        features[f'{metric_name}_std_5m'] = df_resampled['value'].rolling('5min').std()\n        features[f'{metric_name}_min_5m'] = df_resampled['value'].rolling('5min').min()\n        features[f'{metric_name}_max_5m'] = df_resampled['value'].rolling('5min').max()\n        \n        # Add lag features\n        features[f'{metric_name}_lag_1'] = df_resampled['value'].shift(1)\n        features[f'{metric_name}_lag_5'] = df_resampled['value'].shift(5)\n        \n        # Add rate of change\n        features[f'{metric_name}_diff'] = df_resampled['value'].diff()\n        features[f'{metric_name}_pct_change'] = df_resampled['value'].pct_change()\n        \n        # Combine anomaly labels (any metric anomaly = overall anomaly)\n        metric_anomalies = df_resampled['is_anomaly'].fillna(False)\n        labels = labels | metric_anomalies\n    \n    # Fill missing values\n    features = features.ffill().bfill()\n    labels = labels.fillna(False)\n    \n    # Replace infinity values with 0 and remaining NaN with 0\n    features = features.replace([np.inf, -np.inf], 0)\n    features = features.fillna(0)\n    \n    print(f\"  ‚úÖ Feature matrix: {features.shape}\")\n    print(f\"  üè∑Ô∏è Anomaly labels: {labels.sum()} anomalies ({labels.mean():.2%})\")\n    print(f\"  üìê Features per metric: ~9 (value + 4 rolling + 2 lag + 2 diff)\")\n    \n    return features, labels\n\n# Create feature matrix\nX, y = create_feature_matrix(training_data)\n\nprint(f\"\\nüìä Feature Engineering Complete:\")\nprint(f\"  Features: {X.shape[1]} columns\")\nprint(f\"  Samples: {X.shape[0]:,} rows\")\nprint(f\"  Anomaly rate: {y.mean():.2%}\")",
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Train Isolation Forest model and evaluate its performance.\n",
    "\n",
    "**Note:** With enhanced metrics, we now have significantly more features which improves detection accuracy but requires the sklearn Pipeline to handle scaling properly."
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(f\"üìä Data Split:\")\nprint(f\"  Training: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\nprint(f\"  Testing: {X_test.shape[0]:,} samples\")\nprint(f\"  Training anomalies: {y_train.sum()} ({y_train.mean():.2%})\")\nprint(f\"  Testing anomalies: {y_test.sum()} ({y_test.mean():.2%})\")\n\n# ============================================================================\n# CREATE SKLEARN PIPELINE (KServe Compatible)\n# ============================================================================\n# Using Pipeline ensures:\n# 1. Scaler and model are saved together in ONE .pkl file\n# 2. KServe can load and use directly without manual preprocessing\n# 3. Inference is consistent with training\n# ============================================================================\n\nprint(f\"\\nüîß Creating Isolation Forest pipeline...\")\nprint(f\"   Detection focus: {DETECTION_FOCUS.value}\")\nprint(f\"   Config: {ISOLATION_FOREST_CONFIG['n_estimators']} trees, {ISOLATION_FOREST_CONFIG['contamination']*100:.0f}% contamination\")\n\nisolation_forest_pipeline = Pipeline([\n    ('scaler', RobustScaler()),  # More robust to outliers than StandardScaler\n    ('isolation_forest', IsolationForest(**ISOLATION_FOREST_CONFIG))\n])\n\nprint(\"‚úÖ Pipeline created (RobustScaler + Isolation Forest)\")\nprint(f\"   Features: {X_train.shape[1]}\")",
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Isolation Forest Pipeline\nprint(\"üå≤ Training Isolation Forest pipeline...\")\nprint(f\"   Training on {X_train.shape[0]:,} samples with {X_train.shape[1]} features\")\nprint(\"   Pipeline automatically handles: scaler.fit_transform() ‚Üí model.fit()\")\n\nimport time\nstart_time = time.time()\n\n# Fit pipeline on training data\nisolation_forest_pipeline.fit(X_train)\n\ntraining_time = time.time() - start_time\nprint(f\"‚úÖ Training complete in {training_time:.2f} seconds\")\n\n# Make predictions using pipeline\nprint(\"\\nüîÆ Making predictions...\")\ny_pred_train = isolation_forest_pipeline.predict(X_train)\ny_pred_test = isolation_forest_pipeline.predict(X_test)\n\n# Get anomaly scores\ntrain_scores = isolation_forest_pipeline.decision_function(X_train)\ntest_scores = isolation_forest_pipeline.decision_function(X_test)\n\n# Convert predictions to binary (1 = normal, -1 = anomaly)\ny_pred_train_binary = (y_pred_train == -1)\ny_pred_test_binary = (y_pred_test == -1)\n\nprint(f\"  Training predictions: {y_pred_train_binary.sum()} anomalies detected\")\nprint(f\"  Testing predictions: {y_pred_test_binary.sum()} anomalies detected\")\nprint(f\"\\n‚úÖ Pipeline handles scaling automatically - no separate scaler needed!\")",
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate model performance\nprint(\"üìä Model Evaluation\")\nprint(\"=\" * 60)\nprint(f\"Detection Focus: {DETECTION_FOCUS.value.upper()}\")\nprint(f\"Features: {X.shape[1]} | Metrics: {len(TARGET_METRICS)}\")\nprint(\"=\" * 60)\n\n# Training set performance\nprint(\"\\nüèãÔ∏è Training Set Performance:\")\nprint(classification_report(y_train, y_pred_train_binary, \n                          target_names=['Normal', 'Anomaly']))\n\n# Test set performance\nprint(\"\\nüß™ Test Set Performance:\")\nprint(classification_report(y_test, y_pred_test_binary, \n                          target_names=['Normal', 'Anomaly']))\n\n# Confusion matrices\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Training confusion matrix\ncm_train = confusion_matrix(y_train, y_pred_train_binary)\nsns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Normal', 'Anomaly'], \n            yticklabels=['Normal', 'Anomaly'], ax=axes[0])\naxes[0].set_title(f'Training Set Confusion Matrix\\n({len(TARGET_METRICS)} enhanced metrics)')\naxes[0].set_ylabel('True Label')\naxes[0].set_xlabel('Predicted Label')\n\n# Test confusion matrix\ncm_test = confusion_matrix(y_test, y_pred_test_binary)\nsns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Normal', 'Anomaly'], \n            yticklabels=['Normal', 'Anomaly'], ax=axes[1])\naxes[1].set_title(f'Test Set Confusion Matrix\\n(Focus: {DETECTION_FOCUS.value})')\naxes[1].set_ylabel('True Label')\naxes[1].set_xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()",
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Visualization"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze anomaly scores distribution\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle(f'Isolation Forest Analysis ({len(TARGET_METRICS)} Enhanced Metrics)', fontsize=16, fontweight='bold')\n\n# Score distribution\naxes[0, 0].hist(train_scores[~y_train], bins=50, alpha=0.7, label='Normal', density=True)\naxes[0, 0].hist(train_scores[y_train], bins=50, alpha=0.7, label='Anomaly', density=True)\naxes[0, 0].set_title('Anomaly Score Distribution (Training)')\naxes[0, 0].set_xlabel('Anomaly Score')\naxes[0, 0].set_ylabel('Density')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Score vs time (sample)\nsample_size = min(1000, len(test_scores))\nsample_indices = np.random.choice(len(test_scores), sample_size, replace=False)\nsample_indices = np.sort(sample_indices)\n\naxes[0, 1].plot(sample_indices, test_scores[sample_indices], 'b-', alpha=0.7, linewidth=1)\nanomaly_indices = sample_indices[y_test.iloc[sample_indices]]\nif len(anomaly_indices) > 0:\n    axes[0, 1].scatter(anomaly_indices, test_scores[anomaly_indices], \n                      color='red', s=30, alpha=0.8, label='True Anomalies')\naxes[0, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\naxes[0, 1].set_title('Anomaly Scores Over Time (Test Sample)')\naxes[0, 1].set_xlabel('Sample Index')\naxes[0, 1].set_ylabel('Anomaly Score')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Feature importance (using PCA to visualize)\npca = PCA(n_components=2)\nX_test_pca = pca.fit_transform(X_test)\n\nnormal_mask = ~y_test\nanomaly_mask = y_test\n\naxes[1, 0].scatter(X_test_pca[normal_mask, 0], X_test_pca[normal_mask, 1], \n                  c='blue', alpha=0.6, s=20, label='Normal')\naxes[1, 0].scatter(X_test_pca[anomaly_mask, 0], X_test_pca[anomaly_mask, 1], \n                  c='red', alpha=0.8, s=30, label='Anomaly')\naxes[1, 0].set_title('PCA Visualization (Test Set)')\naxes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\naxes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Model performance metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n\nprecision = precision_score(y_test, y_pred_test_binary)\nrecall = recall_score(y_test, y_pred_test_binary)\nf1 = f1_score(y_test, y_pred_test_binary)\n\n# Convert scores to probabilities for AUC calculation\ntest_scores_prob = (test_scores - test_scores.min()) / (test_scores.max() - test_scores.min())\nauc = roc_auc_score(y_test, 1 - test_scores_prob)\n\nmetrics_text = f\"\"\"\nModel Performance Metrics:\n\nPrecision: {precision:.3f}\nRecall: {recall:.3f}\nF1-Score: {f1:.3f}\nAUC-ROC: {auc:.3f}\n\nConfiguration:\nFocus: {DETECTION_FOCUS.value}\nTrees: {ISOLATION_FOREST_CONFIG['n_estimators']}\nContamination: {ISOLATION_FOREST_CONFIG['contamination']}\nFeatures: {X.shape[1]}\nMetrics: {len(TARGET_METRICS)}\n\nData:\nTraining: {X_train.shape[0]:,}\nTesting: {X_test.shape[0]:,}\n\"\"\"\n\naxes[1, 1].text(0.05, 0.95, metrics_text, transform=axes[1, 1].transAxes, \n               fontsize=10, verticalalignment='top',\n               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\naxes[1, 1].set_title('Model Summary')\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüéØ Model Performance Summary:\")\nprint(f\"  Detection Focus: {DETECTION_FOCUS.value}\")\nprint(f\"  Metrics Used: {len(TARGET_METRICS)}\")\nprint(f\"  Features: {X.shape[1]}\")\nprint(f\"  Precision: {precision:.3f}\")\nprint(f\"  Recall: {recall:.3f}\")\nprint(f\"  F1-Score: {f1:.3f}\")\nprint(f\"  AUC-ROC: {auc:.3f}\")",
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "source": "## Save Model and Upload to S3\n\nSave the trained pipeline model in KServe-compatible format.",
   "metadata": {},
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "source": "# Save pipeline model to persistent storage\n# Use /mnt/models for persistent storage (model-storage-pvc)\n# Fallback to local for development outside cluster\nMODELS_DIR = Path('/mnt/models') if Path('/mnt/models').exists() else Path('/opt/app-root/src/models')\n\n# Create KServe-compatible subdirectory structure\nMODEL_NAME = 'anomaly-detector'\nMODEL_DIR = MODELS_DIR / MODEL_NAME\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n# Save with KServe expected filename\nmodel_path = MODEL_DIR / 'model.pkl'\n\n# Migration: Move old flat file if exists\nold_path = MODELS_DIR / 'anomaly-detector.pkl'\nif old_path.exists() and not model_path.exists():\n    import shutil\n    shutil.move(str(old_path), str(model_path))\n    print(f\"üîÑ Migrated model from {old_path} to {model_path}\")\n\n# ‚ú® Save SINGLE pipeline file (KServe compatible)\n# KServe sklearn server expects model at: /mnt/models/anomaly-detector/model.pkl\njoblib.dump(isolation_forest_pipeline, model_path)\nprint(f\"üíæ Saved Isolation Forest pipeline to: {model_path}\")\nprint(f\"   ‚úÖ KServe-compatible path: {MODEL_NAME}/model.pkl\")\nprint(f\"   ‚úÖ Single .pkl file (scaler + model combined)\")\nprint(f\"   ‚úÖ Enhanced metrics: {len(TARGET_METRICS)} metrics, {X.shape[1]} features\")\n\n# Save model metadata (features list for inference)\nimport json\nmetadata = {\n    'model_name': MODEL_NAME,\n    'detection_focus': DETECTION_FOCUS.value,\n    'n_metrics': len(TARGET_METRICS),\n    'n_features': X.shape[1],\n    'metrics': TARGET_METRICS,\n    'feature_names': list(X.columns),\n    'config': {\n        'contamination': ISOLATION_FOREST_CONFIG['contamination'],\n        'n_estimators': ISOLATION_FOREST_CONFIG['n_estimators'],\n    },\n    'performance': {\n        'precision': float(precision),\n        'recall': float(recall),\n        'f1_score': float(f1),\n        'auc_roc': float(auc),\n    },\n    'created_at': datetime.now().isoformat(),\n}\n\nmetadata_path = MODEL_DIR / 'metadata.json'\nwith open(metadata_path, 'w') as f:\n    json.dump(metadata, f, indent=2)\nprint(f\"üìã Saved metadata to: {metadata_path}\")\n\n# Upload model to S3 for persistent storage\ntry:\n    from common_functions import upload_model_to_s3, test_s3_connection\n    \n    if test_s3_connection():\n        upload_model_to_s3(\n            str(model_path),\n            s3_key='models/anomaly-detection/anomaly-detector/model.pkl'\n        )\n        print(f\"‚òÅÔ∏è  Uploaded to S3: models/anomaly-detection/anomaly-detector/model.pkl\")\n        \n        # Also upload metadata\n        upload_model_to_s3(\n            str(metadata_path),\n            s3_key='models/anomaly-detection/anomaly-detector/metadata.json'\n        )\n        print(f\"‚òÅÔ∏è  Uploaded to S3: models/anomaly-detection/anomaly-detector/metadata.json\")\n    else:\n        print(\"‚ö†Ô∏è S3 not available - model saved locally only\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è S3 functions not available - model saved locally only\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è S3 upload failed (non-critical): {e}\")\n\n# Verify model saved\nassert model_path.exists(), \"Pipeline model not saved\"\nprint(\"\\n‚úÖ Model pipeline saved successfully\")\nprint(f\"   Path: {model_path}\")\nprint(f\"   Size: {model_path.stat().st_size / 1024:.2f} KB\")\nprint(f\"   Metadata: {metadata_path}\")\n\n# Clean up old separate model/scaler files if they exist\nold_model = MODELS_DIR / 'isolation_forest_model.pkl'\nold_scaler = MODELS_DIR / 'isolation_forest_scaler.pkl'\nfor old_file in [old_model, old_scaler]:\n    if old_file.exists():\n        old_file.unlink()\n        print(f\"üóëÔ∏è  Removed old file: {old_file.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Enhanced Metrics (v2.0) Changes:\n",
    "\n",
    "| Aspect | Original | Enhanced |\n",
    "|--------|----------|----------|\n",
    "| **Metrics** | 5 | 30+ |\n",
    "| **Features** | ~45 | ~270+ |\n",
    "| **Categories** | 1 (general) | 6 (resource, stability, performance, network, control plane, k8s) |\n",
    "| **Config** | Fixed | Category-specific tuning |\n",
    "\n",
    "### Detection Categories:\n",
    "- **RESOURCE**: CPU/memory exhaustion, throttling\n",
    "- **STABILITY**: Crashes, restarts, OOM kills\n",
    "- **PERFORMANCE**: Latency, throughput degradation\n",
    "- **NETWORK**: Errors, drops, retransmits\n",
    "- **CONTROL_PLANE**: API server, etcd, scheduler\n",
    "\n",
    "### Next Steps:\n",
    "1. Deploy model to KServe for real-time inference\n",
    "2. Connect to Coordination Engine for automated remediation\n",
    "3. Monitor model performance in production\n",
    "4. Retrain periodically with real anomaly data"
   ],
   "id": "cell-15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
